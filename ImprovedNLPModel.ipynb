{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImprovedNLPModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHQEyCuZCi2uR/H1gdYHU0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadkhalid913/ML-Practice/blob/main/ImprovedNLPModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EiXkr2BrX5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157d0a8b-1a74-4a92-9c88-408348d07769"
      },
      "source": [
        "import nltk \n",
        "from nltk.stem import wordnet, WordNetLemmatizer\n",
        "import re \n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "EnglishStopwords = nltk.corpus.stopwords.words(\"english\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AB70vL7ztYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LwdX6Xgxlpp",
        "outputId": "7d0e9e7c-b9ad-40ad-ee19-6caeee18a7db"
      },
      "source": [
        "data = pd.read_csv(\"train.txt\", sep = \";\")\n",
        "x = data.iloc[ : , : -1].values\n",
        "y = data.iloc[ : , -1 : ].values\n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(x,y, test_size=0.1)\n",
        "\n",
        "print(trainX.shape)\n",
        "print(trainY.shape)\n",
        "print(testX.shape)\n",
        "print(testY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14399, 1)\n",
            "(14399, 1)\n",
            "(1600, 1)\n",
            "(1600, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oaLEFsyy6Dn"
      },
      "source": [
        "def CleanFeatures(features):\n",
        "  '''\n",
        "    takes 2D numpy array of text data and \n",
        "    removes stopwords, non-alphanumeric characters,\n",
        "    trailing whitespaces, and applies lemmatization \n",
        "  '''\n",
        "\n",
        "  lemma = WordNetLemmatizer()\n",
        "  sentences = features.flatten()\n",
        "  cleaned = []\n",
        "  for sentence in sentences:\n",
        "      sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "      sentence = sentence.lower()\n",
        "      sentence = sentence.split()\n",
        "      sentence = [lemma.lemmatize(word) for word in sentence if word not in set(EnglishStopwords)]\n",
        "      sentence = \" \".join(sentence)\n",
        "      cleaned.append(sentence)\n",
        "\n",
        "  \n",
        "  return cleaned \n",
        "\n",
        "trainX = CleanFeatures(trainX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW0fSLtW3rRB"
      },
      "source": [
        "def Tokenize(sentences):\n",
        "  ''' \n",
        "    Takes a 1D string of sentences and tokenizes them\n",
        "    with 150 tokens by default\n",
        "  '''\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "  tokenizer.fit_on_texts(sentences)\n",
        "  sequences = tokenizer.texts_to_sequences(sentences)\n",
        "  sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen = 50, dtype='int32')\n",
        "  return sequences, tokenizer\n",
        "def TokenizeTestData(testData, tokenizerObject):\n",
        "  '''\n",
        "    testData: 1D array of sentences\n",
        "  '''\n",
        "  sequences = tokenizerObject.texts_to_sequences(testData)\n",
        "  return tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen = 50, dtype='int32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHoZUdalDxHd"
      },
      "source": [
        "trainX , tokenizer = Tokenize(trainX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSElusmxnq9O"
      },
      "source": [
        "num_words = len(tokenizer.index_word) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HILSwAabCWPf"
      },
      "source": [
        "encoder = OneHotEncoder()\n",
        "trainY = encoder.fit_transform(trainY).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjV8ZoWtBGiX"
      },
      "source": [
        "def CreateModel():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(num_words, 480, input_length=50))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units = 128, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(units = 64, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(units = 6, activation=\"softmax\"))\n",
        "  model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "  return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKnQQ3pBDIgf",
        "outputId": "53f5cd2b-4d7e-441e-d4bc-dd78932c9629"
      },
      "source": [
        "ann = CreateModel()\n",
        "ann.summary()\n",
        "ann.fit(trainX, trainY, epochs = 25, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 480)           6143040   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 24000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               3072128   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,223,814\n",
            "Trainable params: 9,223,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "450/450 [==============================] - 44s 96ms/step - loss: 1.0102 - accuracy: 0.6152\n",
            "Epoch 2/25\n",
            "450/450 [==============================] - 44s 97ms/step - loss: 0.1875 - accuracy: 0.9399\n",
            "Epoch 3/25\n",
            "450/450 [==============================] - 44s 97ms/step - loss: 0.0489 - accuracy: 0.9860\n",
            "Epoch 4/25\n",
            "450/450 [==============================] - 44s 98ms/step - loss: 0.0253 - accuracy: 0.9928\n",
            "Epoch 5/25\n",
            "450/450 [==============================] - 45s 99ms/step - loss: 0.0188 - accuracy: 0.9949\n",
            "Epoch 6/25\n",
            "450/450 [==============================] - 44s 97ms/step - loss: 0.0179 - accuracy: 0.9947\n",
            "Epoch 7/25\n",
            "450/450 [==============================] - 43s 97ms/step - loss: 0.0154 - accuracy: 0.9958\n",
            "Epoch 8/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0151 - accuracy: 0.9953\n",
            "Epoch 9/25\n",
            "450/450 [==============================] - 43s 97ms/step - loss: 0.0219 - accuracy: 0.9933\n",
            "Epoch 10/25\n",
            "450/450 [==============================] - 43s 97ms/step - loss: 0.0321 - accuracy: 0.9891\n",
            "Epoch 11/25\n",
            "450/450 [==============================] - 44s 97ms/step - loss: 0.0194 - accuracy: 0.9921\n",
            "Epoch 12/25\n",
            "450/450 [==============================] - 44s 98ms/step - loss: 0.0131 - accuracy: 0.9951\n",
            "Epoch 13/25\n",
            "450/450 [==============================] - 43s 95ms/step - loss: 0.0104 - accuracy: 0.9958\n",
            "Epoch 14/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0100 - accuracy: 0.9960\n",
            "Epoch 15/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0086 - accuracy: 0.9962\n",
            "Epoch 16/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0132 - accuracy: 0.9953\n",
            "Epoch 17/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0161 - accuracy: 0.9944\n",
            "Epoch 18/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0147 - accuracy: 0.9945\n",
            "Epoch 19/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0123 - accuracy: 0.9944\n",
            "Epoch 20/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0104 - accuracy: 0.9961\n",
            "Epoch 21/25\n",
            "450/450 [==============================] - 43s 96ms/step - loss: 0.0091 - accuracy: 0.9962\n",
            "Epoch 22/25\n",
            "450/450 [==============================] - 44s 97ms/step - loss: 0.0089 - accuracy: 0.9965\n",
            "Epoch 23/25\n",
            "450/450 [==============================] - 44s 97ms/step - loss: 0.0057 - accuracy: 0.9967\n",
            "Epoch 24/25\n",
            "450/450 [==============================] - 43s 97ms/step - loss: 0.0065 - accuracy: 0.9965\n",
            "Epoch 25/25\n",
            "450/450 [==============================] - 44s 97ms/step - loss: 0.0052 - accuracy: 0.9971\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4499ec8710>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-286NRGmK8F3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF_EcKp1xzu4"
      },
      "source": [
        "# ann.save_weights(\"HIGH_PARAM_MODEL\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaf7ulzQQqgj"
      },
      "source": [
        "testX = CleanFeatures(testX)\n",
        "testX = TokenizeTestData(testX, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKe-USc6NNj0"
      },
      "source": [
        "testY = encoder.transform(testY).toarray()\n",
        "np.array(testY).shape\n",
        "np.array(testX).shape\n",
        "result = ann.predict(testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3lWlzxDQIeO",
        "outputId": "76eada6d-714b-450b-8e28-2b5873c0b6c5"
      },
      "source": [
        "# y_acc = encoder.transform(testY)\n",
        "y_truth = np.argmax(testY, axis = 1)\n",
        "y_preds = np.argmax(result, axis = 1)\n",
        "\n",
        "correct_preds = y_preds == y_truth \n",
        "print(np.sum(correct_preds) / 1600)\n",
        "\n",
        "# print(correct_preds)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.83875\n"
          ]
        }
      ]
    }
  ]
}