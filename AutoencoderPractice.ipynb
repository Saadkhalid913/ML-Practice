{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoencoderPractice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPUIEpIVWnAzE67aAEwSFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadkhalid913/ML-Practice/blob/main/AutoencoderPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import os \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "no3jj1GtLsvm"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- CONSTANTS ---- \n",
        "batch_size = 128 \n",
        "image_size = (1,28,28)\n",
        "\n",
        "encoder_input_shape = 784\n",
        "encoder_first_hidden_layer = 400 \n",
        "latent_dimention_size = 6\n"
      ],
      "metadata": {
        "id": "18Ka560u0wvQ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# array of transformation to be applied to the training set \n",
        "transforms = torchvision.transforms.Compose([\n",
        "              torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(root=\"./data\",\n",
        "                                      transform=transforms,\n",
        "                                      train=True,\n",
        "                                      download = True\n",
        "                                      )\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_set,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              )\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(root=\"./data\",\n",
        "                                      transform=transforms,\n",
        "                                      train=False,\n",
        "                                      download = True\n",
        "                                      )\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_set,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              )\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(root=\"./data\",\n",
        "                                      transform=transforms,\n",
        "                                      train=True,\n",
        "                                      download = True\n",
        "                                      )\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_set,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              )\n"
      ],
      "metadata": {
        "id": "poaNrVc3zgcc"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a directory for the results \n",
        "sample_dir = \"results\"\n",
        "if not os.path.exists(sample_dir):\n",
        "  os.mkdir(sample_dir)"
      ],
      "metadata": {
        "id": "Ug9qnaQi2GwP"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self):\n",
        "    # initialize to use all the pytorch features \n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Linear(encoder_input_shape, encoder_first_hidden_layer)\n",
        "    self.log_var = nn.Linear(encoder_first_hidden_layer, latent_dimention_size) # log var (refer to paper)\n",
        "    self.mean = nn.Linear(encoder_first_hidden_layer, latent_dimention_size) # mean of distribution\n",
        "\n",
        "    self.dec_fc1 = nn.Linear(latent_dimention_size, encoder_first_hidden_layer) # (latent dim => hidden dim)\n",
        "    self.dec_fc2 = nn.Linear(encoder_first_hidden_layer, encoder_input_shape) # (hidden dim => 784)\n",
        "\n",
        "\n",
        "  def encode(self, x):\n",
        "    # layer 1 \n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    # Mean and Logvar (for reparamaterization trick)\n",
        "    mu = self.mean(x)\n",
        "    log_var = self.log_var(x)\n",
        "    return (mu, log_var)\n",
        "\n",
        "  def reparameterize(self, mean, log_var):\n",
        "    # refer back for why we do this \n",
        "    standard_deviation = torch.exp(log_var / 2)\n",
        "    noise = torch.randn_like(standard_deviation)\n",
        "    \n",
        "    return mean + standard_deviation * noise \n",
        "\n",
        "  def decode(self, x):\n",
        "    x = self.dec_fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dec_fc2(x)\n",
        "    x = F.sigmoid(x)\n",
        "    return x \n",
        "\n",
        " \n",
        " \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-8ilAoO16sY"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0015)"
      ],
      "metadata": {
        "id": "fDGfdrx5Ak08"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(output, ground_truth, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(input=output, target=ground_truth.view(-1, encoder_input_shape), reduction=\"sum\")\n",
        "    KL_div = 0.5 * torch.sum(log_var.exp() + mu.pow(2) - 1 - log_var, 1)\n",
        "    kld_sum = torch.sum(KL_div)\n",
        "\n",
        "    return BCE + kld_sum\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (image, _) in enumerate(train_dataloader):\n",
        "      mu, log_var = model.encode(image.view(-1, encoder_input_shape))\n",
        "      image = image.to(device)\n",
        "      x = model.reparameterize(mu, log_var)\n",
        "      output = model.decode(x)\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss = loss(output, image, mu, log_var)\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += batch_loss.item()\n",
        "\n",
        "    print(f\"Total loss for Epoch #{epoch} is {total_loss / (len(train_dataloader))}\")\n",
        "    if (epoch % 5 == 0):\n",
        "      test(epoch)\n",
        "\n",
        "\n",
        "def test(epoch: int):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (image, _) in enumerate(test_dataloader):\n",
        "      mu, log_var = model.encode(image.view(-1, encoder_input_shape))\n",
        "      latent = model.reparameterize(mu, log_var)\n",
        "      output = model.decode(latent)\n",
        "      test_loss = loss(output, image, mu, log_var)\n",
        "      total_loss += test_loss.item()\n",
        "      if (i == 0):\n",
        "        comparision = torch.cat([image[: 5], output.view(batch_size, 1, 28,28)[: 5]])\n",
        "\n",
        "        # the image needs to be saved to the cpu before we can save it \n",
        "        torchvision.utils.save_image(comparision.cpu(), \"results/reconstruction_\" + str(epoch) + \".png\", nrow=5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(0,15):\n",
        "  train(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teVZfNXF4kFU",
        "outputId": "3432def3-146e-4855-ea48-ab0334226d18"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss for Epoch #0 is 16082.171518939898\n",
            "Total loss for Epoch #1 is 15887.149834671509\n",
            "Total loss for Epoch #2 is 15729.898906000133\n",
            "Total loss for Epoch #3 is 15603.97225646322\n",
            "Total loss for Epoch #4 is 15500.036241088086\n",
            "Total loss for Epoch #5 is 15415.952868886594\n",
            "Total loss for Epoch #6 is 15337.231678521455\n",
            "Total loss for Epoch #7 is 15268.845559451625\n",
            "Total loss for Epoch #8 is 15209.991785630997\n",
            "Total loss for Epoch #9 is 15157.950938666045\n",
            "Total loss for Epoch #10 is 15107.614986590484\n",
            "Total loss for Epoch #11 is 15069.081167127531\n",
            "Total loss for Epoch #12 is 15034.110959571562\n",
            "Total loss for Epoch #13 is 14994.139546408582\n",
            "Total loss for Epoch #14 is 14965.689625949493\n"
          ]
        }
      ]
    }
  ]
}