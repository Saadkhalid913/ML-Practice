{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadkhalid913/ML-Practice/blob/main/U_Net_(BioMedical_Imaging).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ig_scI79jLd-"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Iq96nCiuou",
        "outputId": "90b49850-c509-4ccd-a515-75370d17164e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms \n",
        "import torch.nn.functional as F \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os \n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image\n",
        "import google.colab.drive as drive \n",
        "from torch.utils.data import Dataset, dataloader\n",
        "import torch\n",
        "import tensorflow as tf \n",
        "import tqdm\n",
        "drive.mount(\"drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK69v7c46YOG",
        "outputId": "5f788176-f79a-401b-a5e2-0db7c9f4e11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BbX4vinNivCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0a74ca-203d-44a2-e6ef-bbeb40f90c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘../root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "# !mkdir ../root/.kaggle\n",
        "# !cp drive/MyDrive/kaggle/kaggle.json ../root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZAw76g4BkWyg"
      },
      "outputs": [],
      "source": [
        "import kaggle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UeNzCa9hmMmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c127697-e420-4fe3-97f6-a817402e7a8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fire-segmentation-image-dataset.zip to /content\n",
            " 99% 465M/472M [00:02<00:00, 212MB/s]\n",
            "100% 472M/472M [00:02<00:00, 205MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !kaggle datasets download \"diversisai/fire-segmentation-image-dataset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip fire-segmentation-image-dataset.zip"
      ],
      "metadata": {
        "id": "Sz8_u1HtGDO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rsi1-Na0rZIg"
      },
      "outputs": [],
      "source": [
        "class UNETDataset(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_dir  = \"Image/Fire\"\n",
        "    self.mask_dir = \"Segmentation_Mask/Fire\"\n",
        "    self.image_names = next(os.walk(self.train_dir))[2]\n",
        "    self.number_of_images = len(self.image_names)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    # return self.number_of_images\n",
        "    return self.number_of_images\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      image_name = self.image_names[index]\n",
        "\n",
        "      image_data = Image.open(f\"{self.train_dir}/{image_name}\")\n",
        "      mask_data = Image.open(f\"{self.mask_dir}/{image_name}\")\n",
        "\n",
        "      tensor_transform = transforms.ToTensor()\n",
        "\n",
        "      img = tensor_transform(image_data)\n",
        "      mask = tensor_transform(mask_data)\n",
        "\n",
        "      return img, mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TW8YujOIqswX"
      },
      "outputs": [],
      "source": [
        "DS = UNETDataset()\n",
        "x,y = DS[12]\n",
        "# fig, ax = plt.subplots(nrows = 1, ncols = 2)\n",
        "\n",
        "# fig.set_figwidth(10)\n",
        "# fig.set_figheight(10)\n",
        "\n",
        "# ax[0].imshow(x.permute(1,2,0))\n",
        "# ax[1].imshow(y.squeeze())\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HJ0pHSpigbh9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mvzuCn7Ny8Ki"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "DS = UNETDataset()\n",
        "train_size = int(0.9 * len(DS))\n",
        "test_size = len(DS) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(DS, [train_size, test_size])\n",
        "\n",
        "train_loader = dataloader.DataLoader(train_set, batch_size = batch_size, shuffle = False)\n",
        "test_loader = dataloader.DataLoader(test_set, batch_size = batch_size, shuffle = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j1oPkxrA0T0M"
      },
      "outputs": [],
      "source": [
        "class UNET(nn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.learning_rate = 1e-5\n",
        "\n",
        "    # batch norm \n",
        "\n",
        "    self.Batch_Norm = nn.BatchNorm2d(num_features=3)\n",
        "\n",
        "    ## L1\n",
        "    self.conv_1_1 = nn.Conv2d(3, 64, 3, 1,padding = \"same\")\n",
        "    self.conv_1_2 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.dropout_1 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_1 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L2\n",
        "    self.conv_2_1 = nn.Conv2d(64, 128, 3, 1,padding = \"same\")\n",
        "    self.conv_2_2 = nn.Conv2d(128, 128, 3, 1, padding = \"same\")\n",
        "    self.dropout_2 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_2 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L3\n",
        "    self.conv_3_1 = nn.Conv2d(128,256, 3, 1,padding = \"same\")\n",
        "    self.conv_3_2 = nn.Conv2d(256, 256, 3, 1, padding = \"same\")\n",
        "    self.dropout_3 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_3 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L4\n",
        "    self.conv_4_1 = nn.Conv2d(256, 512, 3, 1,padding = \"same\")\n",
        "    self.conv_4_2 = nn.Conv2d(512, 512, 3, 1, padding = \"same\")\n",
        "    self.dropout_4 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_4 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L5\n",
        "    self.conv_5_1 = nn.Conv2d(512, 1024, 1,padding = \"same\")\n",
        "    self.conv_5_2 = nn.Conv2d(1024, 1024, 3, 1, padding = \"same\")\n",
        "    self.dropout_5 = nn.Dropout2d(0.2)\n",
        "\n",
        "    ##UP 1\n",
        "    self.deconv1_1 = nn.ConvTranspose2d(1024, 512, 3, padding = 1, stride=2, output_padding = 1)\n",
        "    self.deconv1_2 = nn.Conv2d(1024, 512, 3, 1, padding = \"same\")\n",
        "    self.deconv1_3 = nn.Conv2d(512, 512, 3, 1, padding = \"same\")\n",
        "    self.dropout_6 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 2\n",
        "    self.deconv2_1 = nn.ConvTranspose2d(512, 256, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv2_2 = nn.Conv2d(512, 256, 3, 1, padding = \"same\")\n",
        "    self.deconv2_3 = nn.Conv2d(256, 256, 3, 1, padding = \"same\")\n",
        "    self.dropout_7 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 3\n",
        "    self.deconv3_1 = nn.ConvTranspose2d(256, 128, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv3_2 = nn.Conv2d(256, 128, 3, 1, padding = \"same\")\n",
        "    self.deconv3_3 = nn.Conv2d(128, 128, 3, 1, padding = \"same\")\n",
        "    self.dropout_8 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 4\n",
        "    self.deconv4_1 = nn.ConvTranspose2d(128, 64, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv4_2 = nn.Conv2d(128, 64, 3, 1, padding = 1)\n",
        "    self.deconv4_3 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.dropout_9 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 4\n",
        "    self.deconv5_1 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.deconv5_2 = nn.Conv2d(64, 1, 1, 1, padding = \"same\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.Batch_Norm(x)\n",
        "    L1_1 = self.conv_1_1(x)\n",
        "    L1_1 = F.relu(L1_1)\n",
        "    L1_2 = self.conv_1_2(L1_1)\n",
        "    L1_2 = F.relu(L1_2)\n",
        "    L1_2 = self.dropout_1(L1_2)\n",
        "\n",
        "\n",
        "    L1_MAX, L1_indices = self.max_pool_1(L1_2)\n",
        "\n",
        "    L2_1 = self.conv_2_1(L1_MAX)\n",
        "    L2_1 = F.relu(L2_1)\n",
        "    L2_2 = self.conv_2_2(L2_1)\n",
        "    L2_2 = F.relu(L2_2)\n",
        "    L2_2 = self.dropout_2(L2_2)\n",
        "\n",
        "    L2_MAX, L2_indices = self.max_pool_2(L2_2)\n",
        "\n",
        "    L3_1 = self.conv_3_1(L2_MAX)\n",
        "    L3_1 = F.relu(L3_1)\n",
        "    L3_2 = self.conv_3_2(L3_1)\n",
        "    L3_2 = F.relu(L3_2)\n",
        "    L3_2 = self.dropout_3(L3_2)\n",
        "\n",
        "    L3_MAX, L3_indices = self.max_pool_3(L3_2)\n",
        "\n",
        "    L4_1 = self.conv_4_1(L3_MAX)\n",
        "    L4_1 = F.relu(L4_1)\n",
        "    L4_2 = self.conv_4_2(L4_1)\n",
        "    L4_2 = F.relu(L4_2)\n",
        "    L4_2 = self.dropout_4(L4_2)\n",
        "\n",
        "    L4_MAX, L4_indices = self.max_pool_4(L4_2)\n",
        "\n",
        "    L5_1 = self.conv_5_1(L4_MAX)\n",
        "    L5_1 = F.relu(L5_1)\n",
        "    L5_2 = self.conv_5_2(L5_1)\n",
        "    L5_2 = F.relu(L5_2)\n",
        "    L5_2 = self.dropout_5(L5_2)\n",
        "\n",
        "    ## Expansive Path\n",
        "\n",
        "    D1_1 = self.deconv1_1(L5_2)\n",
        "    skip_connection_1 = torch.cat([L4_2, D1_1], dim = 1)\n",
        "    D1_2 = self.deconv1_2(skip_connection_1)\n",
        "    D1_2 = F.relu(D1_2)\n",
        "    D1_3 = self.deconv1_3(D1_2)\n",
        "    D1_3 = F.relu(D1_3)\n",
        "    D1_3 = self.dropout_6(D1_3)\n",
        "\n",
        "    D2_1 = self.deconv2_1(D1_2)\n",
        "    skip_connection_2 = torch.cat([L3_2, D2_1], dim = 1)\n",
        "    D2_2 = self.deconv2_2(skip_connection_2)\n",
        "    D2_2 = F.relu(D2_2)\n",
        "    D2_3 = self.deconv2_3(D2_2)\n",
        "    D2_3 = F.relu(D2_3)\n",
        "    D2_3 = self.dropout_7(D2_3)\n",
        "\n",
        "    D3_1 = self.deconv3_1(D2_2)\n",
        "    skip_connection_3 = torch.cat([L2_2, D3_1], dim = 1)\n",
        "    D3_2 = self.deconv3_2(skip_connection_3)\n",
        "    D3_2 = F.relu(D3_2)\n",
        "    D3_3 = self.deconv3_3(D3_2)\n",
        "    D3_3 = F.relu(D3_3)\n",
        "    D3_3 = self.dropout_8(D3_3)\n",
        "\n",
        "    D4_1 = self.deconv4_1(D3_2)\n",
        "    skip_connection_4 = torch.cat([L1_2, D4_1], dim = 1)\n",
        "    D4_2 = self.deconv4_2(skip_connection_4)\n",
        "    D4_2 = F.relu(D4_2)\n",
        "    D4_3 = self.deconv4_3(D4_2)\n",
        "    D4_3 = F.relu(D4_3)\n",
        "    D4_3 = self.dropout_9(D4_3)\n",
        "\n",
        "    D5_1 = self.deconv5_1(D4_2)\n",
        "    D5_2 = self.deconv5_2(D5_1)\n",
        "    D5_2 = torch.sigmoid(D5_2)\n",
        "\n",
        "\n",
        "\n",
        "    return D5_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7OjU9KVCga8f"
      },
      "outputs": [],
      "source": [
        "NET = UNET()\n",
        "NET = NET.to(device)\n",
        "Optimizer = torch.optim.Adam(NET.parameters(), lr = NET.learning_rate)\n",
        "Loss = nn.BCELoss()\n",
        "\n",
        "EPOCHS = 2\n",
        "num_batches = 100 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfzdiEie0Ctd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbc484c-df2e-43d9-b65d-214bd517db96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Number:  1  /  1545  LOSS: 0\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  Epoch_loss = 0\n",
        "  group_loss = 0\n",
        "  for i, (x,y) in enumerate(train_loader):\n",
        "\n",
        "    if (i % 50 == 0):\n",
        "      print(\"Batch Number: \", i + 1, \" / \", len(train_loader), \" LOSS: \" + str(group_loss))\n",
        "      group_loss = 0 \n",
        "    \n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = NET(x)\n",
        "\n",
        "\n",
        "    Optimizer.zero_grad()\n",
        "    loss = Loss(output, y)\n",
        "    loss.backward()\n",
        "    Optimizer.step()\n",
        "\n",
        "\n",
        "    Epoch_loss += loss.item()\n",
        "    group_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch #{epoch + 1 } complete, LOSS: {Epoch_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET.eval()\n",
        "total_loss = 0\n",
        "for i, (x,y) in enumerate(test_loader):\n",
        "\n",
        "    if (i % 5 == 0):\n",
        "      print(\"Batch Number: \", i + 1, \" / \", len(test_loader), \" LOSS: \" + str(total_loss))\n",
        "      group_loss = 0 \n",
        "    \n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = NET(x)\n",
        "\n",
        "    loss = Loss(output, y)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "print(\"Loss: \" + total_loss)\n"
      ],
      "metadata": {
        "id": "tjDcHJ-evtvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for x,y in test_loader:\n",
        "  fig, ax = plt.subplots(nrows = 16, ncols = 2)\n",
        "  fig.set_figwidth(10)\n",
        "  fig.set_figheight(10)\n",
        "  output = NET(x.to(device))\n",
        "  for i in range(output.shape(0)):\n",
        "    pixels = output[i].permute(1,2,0).squeeze().cpu().detach().numpy()\n",
        "    ax[0, i].imshow(pixels)\n",
        "    ax[1, i].imshow(y[i].squeeze())\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  break\n",
        "\n"
      ],
      "metadata": {
        "id": "sL9UG0X5dobd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "U-Net (BioMedical Imaging)",
      "provenance": [],
      "authorship_tag": "ABX9TyNQulpCAFc7b1kvtJNb52Iy",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}