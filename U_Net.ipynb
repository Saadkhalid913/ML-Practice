{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadkhalid913/ML-Practice/blob/main/U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "ig_scI79jLd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e17e4f-af44-4ea4-a963-0af7d069a283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Iq96nCiuou",
        "outputId": "d8d61659-398b-43c9-f428-c502f9edee20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms \n",
        "import torch.nn.functional as F \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os \n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image\n",
        "import google.colab.drive as drive \n",
        "from torch.utils.data import Dataset, dataloader\n",
        "import torch\n",
        "import tensorflow as tf \n",
        "import tqdm\n",
        "from skimage.filters import threshold_multiotsu\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "import random\n",
        "\n",
        "\n",
        "drive.mount(\"drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK69v7c46YOG",
        "outputId": "1b450f3c-5deb-409a-89be-fbc2bb43cb19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "BbX4vinNivCT"
      },
      "outputs": [],
      "source": [
        "# !mkdir ../root/.kaggle\n",
        "# !cp drive/MyDrive/kaggle/kaggle.json ../root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "ZAw76g4BkWyg"
      },
      "outputs": [],
      "source": [
        "import kaggle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "UeNzCa9hmMmx"
      },
      "outputs": [],
      "source": [
        "# !kaggle datasets download \"dansbecker/cityscapes-image-pairs\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "P7ULw8c8n45W"
      },
      "outputs": [],
      "source": [
        "# !unzip cityscapes-image-pairs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "rsi1-Na0rZIg"
      },
      "outputs": [],
      "source": [
        "\n",
        "n_classes = 5\n",
        "class UNETDataset(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_dir  = \"cityscapes_data/train\"\n",
        "    self.image_names = next(os.walk(self.train_dir))[2][: 10000]\n",
        "    self.number_of_images = len(self.image_names)\n",
        "    self.km = None \n",
        "    self.k_classes = 8\n",
        "    self.train_kmeans()\n",
        "\n",
        "  def getSegmentationArr(self, image, width=256, height=256):\n",
        "      seg_labels = np.zeros((self.k_classes, height, width))\n",
        "      \n",
        "      for c in range(self.k_classes):\n",
        "          seg_labels[c, :, :] = (image == c).astype(int)\n",
        "      return seg_labels\n",
        "\n",
        "  def train_kmeans(self):\n",
        "    colors = []\n",
        "    images = []\n",
        "    n_images = 15\n",
        "    for i in range(n_images):\n",
        "      images.append(self.get_mask(random.randint(0, len(self) - 1)))\n",
        "    for image in images:\n",
        "        colors.append(torch.reshape(image, (image.shape[1]*image.shape[2], 3)).numpy())\n",
        "    colors = np.array(colors, dtype = float)\n",
        "\n",
        "    colors = colors.reshape(n_images * colors.shape[1], 3)\n",
        "    km = KMeans(self.k_classes)\n",
        "    km.fit(colors)\n",
        "    self.km = km \n",
        "\n",
        "  def EncodeImage(self, img: torch.tensor) -> torch.tensor:\n",
        "    height = img.shape[1]\n",
        "    width = img.shape[2]\n",
        "    img = img.reshape(3, height * width).T\n",
        "    output = self.km.predict(img)\n",
        "    predicted_classes = output.reshape(height, width)\n",
        "    return predicted_classes\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.number_of_images\n",
        "\n",
        "  def get_mask(self, index):\n",
        "      image_name = self.image_names[index]\n",
        "      image_data = Image.open(f\"{self.train_dir}/{image_name}\")\n",
        "\n",
        "      tensor_transform = transforms.ToTensor()\n",
        "      image_data = tensor_transform(image_data)\n",
        "\n",
        "      mask = image_data[: , : , 256 : ]\n",
        "\n",
        "      return  mask\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      image_name = self.image_names[index]\n",
        "      image_data = Image.open(f\"{self.train_dir}/{image_name}\")\n",
        "\n",
        "      tensor_transform = transforms.ToTensor()\n",
        "      image_data = tensor_transform(image_data)\n",
        "\n",
        "      img = image_data[: , : ,  : 256 ]\n",
        "      mask = image_data[: , : , 256 : ]\n",
        "\n",
        "      segmented_mask = self.EncodeImage(mask)\n",
        "      segmented_mask = self.getSegmentationArr(segmented_mask)\n",
        "      segmented_mask = torch.tensor(segmented_mask)\n",
        "\n",
        "      return img.float(), segmented_mask.float(), mask.float()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "mvzuCn7Ny8Ki"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_set = UNETDataset()\n",
        "train_loader = dataloader.DataLoader(train_set, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, mask, original = train_set[13]"
      ],
      "metadata": {
        "id": "Uffoy013Skoz"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRnMpR5Ldgg1",
        "outputId": "8a477e6b-c4ca-40d9-cf72-f021350ade1b"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ColorSegmentedImage(img):\n",
        "  colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255), \n",
        "              (0,255,255), (200,50,0),(50,200,0), (50,0,200), \n",
        "              (200,200,50), (0,50,200), (0,200,50), (0,0,0)]\n",
        "\n",
        "  # colors = sns.color_palette(\"hls\", img.shape[0])\n",
        "  mask = np.zeros((3, 256,256), dtype = np.float32)\n",
        "\n",
        "  for i in range(img.shape[0]):\n",
        "    indices = (img[i] == 1)\n",
        "    mask[0][indices] = colors[i][0]\n",
        "    mask[1][indices] = colors[i][1]\n",
        "    mask[2][indices] = colors[i][2]\n",
        "  \n",
        "  mask = mask/255.0\n",
        "  return mask\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(nrows = 1, ncols = 2)\n",
        "# ax[0].imshow(ColorSegmentedImage(mask).transpose(1,2,0))\n",
        "# ax[1].imshow(original.permute(1,2,0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "tYN4uwVacNaa",
        "outputId": "d1f25f07-7d3a-46d3-ad0e-e52267059b9c"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "j1oPkxrA0T0M"
      },
      "outputs": [],
      "source": [
        "class UNET(nn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.learning_rate = 5e-5\n",
        "\n",
        "    # batch norm \n",
        "\n",
        "    self.Batch_Norm = nn.BatchNorm2d(num_features=3)\n",
        "\n",
        "    ## L1\n",
        "    self.conv_1_1 = nn.Conv2d(3, 64, 3, 1,padding = \"same\")\n",
        "    self.conv_1_2 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.dropout_1 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_1 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L2\n",
        "    self.conv_2_1 = nn.Conv2d(64, 128, 3, 1,padding = \"same\")\n",
        "    self.conv_2_2 = nn.Conv2d(128, 128, 3, 1, padding = \"same\")\n",
        "    self.dropout_2 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_2 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L3\n",
        "    self.conv_3_1 = nn.Conv2d(128,256, 3, 1,padding = \"same\")\n",
        "    self.conv_3_2 = nn.Conv2d(256, 256, 3, 1, padding = \"same\")\n",
        "    self.dropout_3 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_3 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L4\n",
        "    self.conv_4_1 = nn.Conv2d(256, 512, 3, 1,padding = \"same\")\n",
        "    self.conv_4_2 = nn.Conv2d(512, 512, 3, 1, padding = \"same\")\n",
        "    self.dropout_4 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_4 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L5\n",
        "    self.conv_5_1 = nn.Conv2d(512, 1024, 1,padding = \"same\")\n",
        "    self.conv_5_2 = nn.Conv2d(1024, 1024, 3, 1, padding = \"same\")\n",
        "    self.dropout_5 = nn.Dropout2d(0.2)\n",
        "\n",
        "    ##UP 1\n",
        "    self.deconv1_1 = nn.ConvTranspose2d(1024, 512, 3, padding = 1, stride=2, output_padding = 1)\n",
        "    self.deconv1_2 = nn.Conv2d(1024, 512, 3, 1, padding = \"same\")\n",
        "    self.deconv1_3 = nn.Conv2d(512, 512, 3, 1, padding = \"same\")\n",
        "    self.dropout_6 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 2\n",
        "    self.deconv2_1 = nn.ConvTranspose2d(512, 256, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv2_2 = nn.Conv2d(512, 256, 3, 1, padding = \"same\")\n",
        "    self.deconv2_3 = nn.Conv2d(256, 256, 3, 1, padding = \"same\")\n",
        "    self.dropout_7 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 3\n",
        "    self.deconv3_1 = nn.ConvTranspose2d(256, 128, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv3_2 = nn.Conv2d(256, 128, 3, 1, padding = \"same\")\n",
        "    self.deconv3_3 = nn.Conv2d(128, 128, 3, 1, padding = \"same\")\n",
        "    self.dropout_8 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 4\n",
        "    self.deconv4_1 = nn.ConvTranspose2d(128, 64, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv4_2 = nn.Conv2d(128, 64, 3, 1, padding = 1)\n",
        "    self.deconv4_3 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.dropout_9 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 4\n",
        "    self.deconv5_1 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.deconv5_2 = nn.Conv2d(64, 8, 1, 1, padding = \"same\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.Batch_Norm(x)\n",
        "    L1_1 = self.conv_1_1(x)\n",
        "    L1_1 = F.relu(L1_1)\n",
        "    L1_2 = self.conv_1_2(L1_1)\n",
        "    L1_2 = F.relu(L1_2)\n",
        "    L1_2 = self.dropout_1(L1_2)\n",
        "\n",
        "\n",
        "    L1_MAX, L1_indices = self.max_pool_1(L1_2)\n",
        "\n",
        "    L2_1 = self.conv_2_1(L1_MAX)\n",
        "    L2_1 = F.relu(L2_1)\n",
        "    L2_2 = self.conv_2_2(L2_1)\n",
        "    L2_2 = F.relu(L2_2)\n",
        "    L2_2 = self.dropout_2(L2_2)\n",
        "\n",
        "    L2_MAX, L2_indices = self.max_pool_2(L2_2)\n",
        "\n",
        "    L3_1 = self.conv_3_1(L2_MAX)\n",
        "    L3_1 = F.relu(L3_1)\n",
        "    L3_2 = self.conv_3_2(L3_1)\n",
        "    L3_2 = F.relu(L3_2)\n",
        "    L3_2 = self.dropout_3(L3_2)\n",
        "\n",
        "    L3_MAX, L3_indices = self.max_pool_3(L3_2)\n",
        "\n",
        "    L4_1 = self.conv_4_1(L3_MAX)\n",
        "    L4_1 = F.relu(L4_1)\n",
        "    L4_2 = self.conv_4_2(L4_1)\n",
        "    L4_2 = F.relu(L4_2)\n",
        "    L4_2 = self.dropout_4(L4_2)\n",
        "\n",
        "    L4_MAX, L4_indices = self.max_pool_4(L4_2)\n",
        "\n",
        "    L5_1 = self.conv_5_1(L4_MAX)\n",
        "    L5_1 = F.relu(L5_1)\n",
        "    L5_2 = self.conv_5_2(L5_1)\n",
        "    L5_2 = F.relu(L5_2)\n",
        "    L5_2 = self.dropout_5(L5_2)\n",
        "\n",
        "    ## Expansive Path\n",
        "\n",
        "    D1_1 = self.deconv1_1(L5_2)\n",
        "    skip_connection_1 = torch.cat([L4_2, D1_1], dim = 1)\n",
        "    D1_2 = self.deconv1_2(skip_connection_1)\n",
        "    D1_2 = F.relu(D1_2)\n",
        "    D1_3 = self.deconv1_3(D1_2)\n",
        "    D1_3 = F.relu(D1_3)\n",
        "    D1_3 = self.dropout_6(D1_3)\n",
        "\n",
        "    D2_1 = self.deconv2_1(D1_2)\n",
        "    skip_connection_2 = torch.cat([L3_2, D2_1], dim = 1)\n",
        "    D2_2 = self.deconv2_2(skip_connection_2)\n",
        "    D2_2 = F.relu(D2_2)\n",
        "    D2_3 = self.deconv2_3(D2_2)\n",
        "    D2_3 = F.relu(D2_3)\n",
        "    D2_3 = self.dropout_7(D2_3)\n",
        "\n",
        "    D3_1 = self.deconv3_1(D2_2)\n",
        "    skip_connection_3 = torch.cat([L2_2, D3_1], dim = 1)\n",
        "    D3_2 = self.deconv3_2(skip_connection_3)\n",
        "    D3_2 = F.relu(D3_2)\n",
        "    D3_3 = self.deconv3_3(D3_2)\n",
        "    D3_3 = F.relu(D3_3)\n",
        "    D3_3 = self.dropout_8(D3_3)\n",
        "\n",
        "    D4_1 = self.deconv4_1(D3_2)\n",
        "    skip_connection_4 = torch.cat([L1_2, D4_1], dim = 1)\n",
        "    D4_2 = self.deconv4_2(skip_connection_4)\n",
        "    D4_2 = F.relu(D4_2)\n",
        "    D4_3 = self.deconv4_3(D4_2)\n",
        "    D4_3 = F.relu(D4_3)\n",
        "    D4_3 = self.dropout_9(D4_3)\n",
        "\n",
        "    D5_1 = self.deconv5_1(D4_2)\n",
        "    D5_2 = self.deconv5_2(D5_1)\n",
        "    D5_2 = torch.sigmoid(D5_2)\n",
        "\n",
        "\n",
        "\n",
        "    return D5_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "7OjU9KVCga8f"
      },
      "outputs": [],
      "source": [
        "NET = UNET()\n",
        "NET = NET.to(device)\n",
        "Optimizer = torch.optim.Adam(NET.parameters(), lr = NET.learning_rate)\n",
        "Loss = nn.BCELoss()\n",
        "\n",
        "EPOCHS = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "KfzdiEie0Ctd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "outputId": "b52cf709-1b97-488f-97b7-9640d31a5f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Number:  1  /  372  LOSS: 0\n",
            "Batch Number:  11  /  372  LOSS: 3.7976526021957397\n",
            "Batch Number:  21  /  372  LOSS: 3.736137479543686\n",
            "Batch Number:  31  /  372  LOSS: 3.5595953166484833\n",
            "Batch Number:  41  /  372  LOSS: 3.300757497549057\n",
            "Batch Number:  51  /  372  LOSS: 3.074609488248825\n",
            "Batch Number:  61  /  372  LOSS: 3.0338334441184998\n",
            "Batch Number:  71  /  372  LOSS: 2.959762156009674\n",
            "Batch Number:  81  /  372  LOSS: 2.8505719900131226\n",
            "Batch Number:  91  /  372  LOSS: 2.854505091905594\n",
            "Batch Number:  101  /  372  LOSS: 2.7839860916137695\n",
            "Batch Number:  111  /  372  LOSS: 2.779235363006592\n",
            "Batch Number:  121  /  372  LOSS: 2.7172687351703644\n",
            "Batch Number:  131  /  372  LOSS: 2.7164508402347565\n",
            "Batch Number:  141  /  372  LOSS: 2.7066992819309235\n",
            "Batch Number:  151  /  372  LOSS: 2.737885594367981\n",
            "Batch Number:  161  /  372  LOSS: 2.6506654620170593\n",
            "Batch Number:  171  /  372  LOSS: 2.720531851053238\n",
            "Batch Number:  181  /  372  LOSS: 2.679611712694168\n",
            "Batch Number:  191  /  372  LOSS: 2.6167699992656708\n",
            "Batch Number:  201  /  372  LOSS: 2.6301510334014893\n",
            "Batch Number:  211  /  372  LOSS: 2.678321585059166\n",
            "Batch Number:  221  /  372  LOSS: 2.59527687728405\n",
            "Batch Number:  231  /  372  LOSS: 2.6569387167692184\n",
            "Batch Number:  241  /  372  LOSS: 2.554053872823715\n",
            "Batch Number:  251  /  372  LOSS: 2.5866788625717163\n",
            "Batch Number:  261  /  372  LOSS: 2.6141444742679596\n",
            "Batch Number:  271  /  372  LOSS: 2.5214930921792984\n",
            "Batch Number:  281  /  372  LOSS: 2.459667071700096\n",
            "Batch Number:  291  /  372  LOSS: 2.526523843407631\n",
            "Batch Number:  301  /  372  LOSS: 2.4070082753896713\n",
            "Batch Number:  311  /  372  LOSS: 2.398394227027893\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-277-2842417d839d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mEpoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mgroup_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "NET.train()\n",
        "for epoch in range(EPOCHS):\n",
        "  Epoch_loss = 0\n",
        "  group_loss = 0\n",
        "  for i, (x,y, original) in enumerate(train_loader):\n",
        "\n",
        "    if (i % 10 == 0):\n",
        "      print(\"Batch Number: \", i + 1, \" / \", len(train_loader), \" LOSS: \" + str(group_loss))\n",
        "      group_loss = 0 \n",
        "    \n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = NET(x)\n",
        "\n",
        "    Optimizer.zero_grad()\n",
        "    loss = Loss(output, y)\n",
        "    loss.backward()\n",
        "    Optimizer.step()\n",
        "\n",
        "    Epoch_loss += loss.item()\n",
        "    group_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch #{epoch + 1 } complete, LOSS: {Epoch_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwW_V-ChL93w"
      },
      "outputs": [],
      "source": [
        "class UNETDatasetTest(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_dir  = \"cityscapes_data/val\"\n",
        "    self.image_names = next(os.walk(self.train_dir))[2]\n",
        "    self.number_of_images = len(self.image_names)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.number_of_images\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      image_name = self.image_names[index]\n",
        "      image_data = Image.open(f\"{self.train_dir}/{image_name}\")\n",
        "\n",
        "      tensor_transform = transforms.ToTensor()\n",
        "      resize_transform = transforms.Resize((64,128))\n",
        "\n",
        "      image_data = tensor_transform(image_data)\n",
        "      image_data = resize_transform(image_data)\n",
        "\n",
        "      # img = image_data[: , : ,  : 256 ]\n",
        "      # mask = image_data[: , : , 256 : ]\n",
        "\n",
        "      img = image_data[: , : ,  : 64 ]\n",
        "      mask = image_data[: , : , 64 : ]\n",
        "\n",
        "      return img, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcvlzWgXMfo-"
      },
      "outputs": [],
      "source": [
        "# torch.save(NET.state_dict, \"/saved_UNET_64x64\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTC0z-S1MFNR"
      },
      "outputs": [],
      "source": [
        "testset = UNETDatasetTest()\n",
        "NET.eval()\n",
        "\n",
        "x,y = testset[34]\n",
        "x = x.unsqueeze(dim = 0)\n",
        "output = NET(x.to(device))\n",
        "img = output.squeeze(dim = 0).permute(1,2,0).detach().cpu().numpy()\n",
        "# img = x.permute(1,2,0).detach().numpy()\n",
        "mask = y.permute(1,2,0).detach().numpy()\n",
        "\n",
        "pixels = np.concatenate((img, mask), axis = 1)\n",
        "plt.imshow(pixels)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "U-Net",
      "provenance": [],
      "authorship_tag": "ABX9TyMY3pPIrHPQul7Y+kp4SkJD",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}