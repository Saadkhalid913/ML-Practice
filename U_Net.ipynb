{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadkhalid913/ML-Practice/blob/main/U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ig_scI79jLd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9281f866-2b6c-4957-81f7-e34d5664a927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Iq96nCiuou",
        "outputId": "9a7a3c7f-d254-44ee-b997-e199e951a619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms \n",
        "import torch.nn.functional as F \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os \n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image\n",
        "import google.colab.drive as drive \n",
        "from torch.utils.data import Dataset, dataloader\n",
        "import torch\n",
        "import tensorflow as tf \n",
        "import tqdm\n",
        "from skimage.filters import threshold_multiotsu\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "drive.mount(\"drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK69v7c46YOG",
        "outputId": "e46e3db0-0a51-4ec2-ff8a-5c608378729f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BbX4vinNivCT"
      },
      "outputs": [],
      "source": [
        "# !mkdir ../root/.kaggle\n",
        "# !cp drive/MyDrive/kaggle/kaggle.json ../root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZAw76g4BkWyg"
      },
      "outputs": [],
      "source": [
        "import kaggle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UeNzCa9hmMmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae356fbf-a071-4a61-88e2-407d742ad499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cityscapes-image-pairs.zip to /content\n",
            " 97% 196M/202M [00:06<00:00, 30.6MB/s]\n",
            "100% 202M/202M [00:06<00:00, 31.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !kaggle datasets download \"dansbecker/cityscapes-image-pairs\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7ULw8c8n45W"
      },
      "outputs": [],
      "source": [
        "# !unzip cityscapes-image-pairs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "rsi1-Na0rZIg"
      },
      "outputs": [],
      "source": [
        "colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255), \n",
        "              (0,255,255), (255,255,255), (200,50,0),(50,200,0), (50,0,200), \n",
        "              (200,200,50), (0,50,200), (0,200,50), (0,0,0)]\n",
        "n_classes = 5\n",
        "class UNETDataset(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_dir  = \"cityscapes_data/train\"\n",
        "    self.image_names = next(os.walk(self.train_dir))[2][: 10000]\n",
        "    self.number_of_images = len(self.image_names)\n",
        "    self.km = None \n",
        "\n",
        "  @staticmethod\n",
        "  def ColorSegmentedImage(img, n_classes):\n",
        "    mask = np.zeros((3, 256,256), dtype = float)\n",
        "\n",
        "    for i in range(n_classes):\n",
        "      indices = (img[i] == 1)\n",
        "      mask[0][indices] = colors[i][0]\n",
        "      mask[1][indices] = colors[i][1]\n",
        "      mask[2][indices] = colors[i][2]\n",
        "    return mask\n",
        "\n",
        "  @staticmethod\n",
        "  def bin_image(mask):\n",
        "      bins = np.arange(0,1,0.2, dtype=float)\n",
        "      new_mask = np.digitize(mask, bins)\n",
        "      return new_mask\n",
        "\n",
        "  @staticmethod\n",
        "  def getSegmentationArr(image, classes, width=256, height=256):\n",
        "      seg_labels = np.zeros((classes, height, width))\n",
        "      img = image[1, :, :,]\n",
        "\n",
        "      for c in range(classes):\n",
        "          seg_labels[c, :, :] = (img == c).astype(int)\n",
        "      return seg_labels\n",
        "\n",
        "  def train_kmeans(self, K=13):\n",
        "    colors = []\n",
        "    images = []\n",
        "    for i in range(50):\n",
        "      images.append(self[i][1])\n",
        "    for image in images:\n",
        "        colors.append(torch.reshape(image, (image.shape[1]*image.shape[2], 3)).numpy())\n",
        "    colors = np.array(colors, dtype = float)\n",
        "\n",
        "    print(\"Training K means on colors data of shape :\", colors.shape)\n",
        "    km = KMeans(K)\n",
        "    km.fit(colors)\n",
        "    # print(\"\\nK-means clustering trained\\n\", km)\n",
        "    # self.km = km \n",
        "\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.number_of_images\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      image_name = self.image_names[index]\n",
        "      image_data = Image.open(f\"{self.train_dir}/{image_name}\")\n",
        "\n",
        "      tensor_transform = transforms.ToTensor()\n",
        "      image_data = tensor_transform(image_data)\n",
        "\n",
        "      img = image_data[: , : ,  : 256 ]\n",
        "      mask = image_data[: , : , 256 : ]\n",
        "\n",
        "      # binned_mask = self.bin_image(mask)\n",
        "      # segmented_mask = self.getSegmentationArr(binned_mask, classes = n_classes, width = 256, height = 256)\n",
        "\n",
        "\n",
        "      # img = image_data[: , : ,  : 128 ]\n",
        "      # mask = image_data[: , : , 128 : ]\n",
        "\n",
        "      return img, mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mvzuCn7Ny8Ki",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "c02ab69a-50ed-44c1-97fb-ab6e41710e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training K means on colors data of shape : (50, 65536, 3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-8188ec5703f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNETDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-a951a0129922>\u001b[0m in \u001b[0;36mtrain_kmeans\u001b[0;34m(self, K)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training K means on colors data of shape :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m# print(\"\\nK-means clustering trained\\n\", km)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# self.km = km\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         )\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m             raise ValueError(\n\u001b[1;32m    795\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m             )\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "train_set = UNETDataset()\n",
        "train_set.train_kmeans()\n",
        "train_loader = dataloader.DataLoader(train_set, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def LayersToRGBImage(img):\n",
        "#     colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255), \n",
        "#               (0,255,255), (255,255,255), (200,50,0),(50,200,0), (50,0,200), \n",
        "#               (200,200,50), (0,50,200), (0,200,50), (0,0,0)]\n",
        "    \n",
        "#     nimg = np.zeros((3, img.shape[0], img.shape[1]))\n",
        "#     for i in range(img.shape[0]):\n",
        "#         c = img[i,:,:]\n",
        "#         col = colors[i]\n",
        "        \n",
        "#         for color_channel in range(3):\n",
        "#             nimg[color_channel, :,:] += col[color_channel]*c\n",
        "#     nimg = nimg/255.0\n",
        "#     return nimg\n",
        "\n",
        "\n",
        "\n",
        "for (x,y, masks) in train_loader:\n",
        "\n",
        "  mask = y[0]\n",
        "  print(masks.shape)\n",
        "  output = LayersToRGBImage(mask)\n",
        "  print(output.shape)\n",
        "  # k_means = KMeans(13)\n",
        "  # tensor_colors = torch.tensor(colors, )\n",
        "\n",
        "\n",
        "  # fig, ax = plt.subplots(nrows = x.shape[0], ncols = 2)\n",
        "  # fig.set_figheight(128)\n",
        "  # fig.set_figwidth(8)\n",
        "  # for i in range(x.shape[0]):\n",
        "  #   img = x[i]\n",
        "  #   segmented = y[i]\n",
        "  #   mask = masks[i]\n",
        "  #   coloured_seg = train_set.ColorSegmentedImage(segmented, n_classes)\n",
        "  #   ax[i,0].imshow(mask.permute(1,2,0))\n",
        "  #   ax[i,1].imshow(coloured_seg.transpose(1,2,0))\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "tYN4uwVacNaa",
        "outputId": "7f64e9ec-d981-44cf-d4e4-78f54e26bc80"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 256, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-74896a3ca925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayersToRGBImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# k_means = KMeans(13)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-74896a3ca925>\u001b[0m in \u001b[0;36mLayersToRGBImage\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolor_channel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mnimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolor_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolor_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mnimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnimg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (256,256) into shape (5,256)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MXA_B0vZXeFQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "PBQjiQLVAcy_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img = np.zeros((3, 256,256))\n",
        "# img[[0], : 128, : 256] = np.full([1,128,256], fill_value = 255)\n",
        "# img[[2], 128: , : 256] = np.full([1,128,256], fill_value = 255)\n",
        "# # plt.imshow(img.transpose(1,2,0))\n",
        "\n",
        "# img = torch.tensor(img)\n",
        "# img = img.permute(1,2,0)\n",
        "# img = img.reshape(-1, img.shape[2])\n",
        "# # print(img[:10])\n",
        "# # print(torch.unique(img, dim=0, return_counts = True))\n"
      ],
      "metadata": {
        "id": "BbPjR6jHO6sM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j1oPkxrA0T0M"
      },
      "outputs": [],
      "source": [
        "class UNET(nn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.learning_rate = 1e-4\n",
        "\n",
        "    # batch norm \n",
        "\n",
        "    self.Batch_Norm = nn.BatchNorm2d(num_features=3)\n",
        "\n",
        "    ## L1\n",
        "    self.conv_1_1 = nn.Conv2d(3, 64, 3, 1,padding = \"same\")\n",
        "    self.conv_1_2 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.dropout_1 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_1 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L2\n",
        "    self.conv_2_1 = nn.Conv2d(64, 128, 3, 1,padding = \"same\")\n",
        "    self.conv_2_2 = nn.Conv2d(128, 128, 3, 1, padding = \"same\")\n",
        "    self.dropout_2 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_2 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L3\n",
        "    self.conv_3_1 = nn.Conv2d(128,256, 3, 1,padding = \"same\")\n",
        "    self.conv_3_2 = nn.Conv2d(256, 256, 3, 1, padding = \"same\")\n",
        "    self.dropout_3 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_3 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L4\n",
        "    self.conv_4_1 = nn.Conv2d(256, 512, 3, 1,padding = \"same\")\n",
        "    self.conv_4_2 = nn.Conv2d(512, 512, 3, 1, padding = \"same\")\n",
        "    self.dropout_4 = nn.Dropout2d(0.2)\n",
        "\n",
        "    self.max_pool_4 = nn.MaxPool2d(kernel_size = 2, return_indices = True)\n",
        "\n",
        "    ## L5\n",
        "    self.conv_5_1 = nn.Conv2d(512, 1024, 1,padding = \"same\")\n",
        "    self.conv_5_2 = nn.Conv2d(1024, 1024, 3, 1, padding = \"same\")\n",
        "    self.dropout_5 = nn.Dropout2d(0.2)\n",
        "\n",
        "    ##UP 1\n",
        "    self.deconv1_1 = nn.ConvTranspose2d(1024, 512, 3, padding = 1, stride=2, output_padding = 1)\n",
        "    self.deconv1_2 = nn.Conv2d(1024, 512, 3, 1, padding = \"same\")\n",
        "    self.deconv1_3 = nn.Conv2d(512, 512, 3, 1, padding = \"same\")\n",
        "    self.dropout_6 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 2\n",
        "    self.deconv2_1 = nn.ConvTranspose2d(512, 256, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv2_2 = nn.Conv2d(512, 256, 3, 1, padding = \"same\")\n",
        "    self.deconv2_3 = nn.Conv2d(256, 256, 3, 1, padding = \"same\")\n",
        "    self.dropout_7 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 3\n",
        "    self.deconv3_1 = nn.ConvTranspose2d(256, 128, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv3_2 = nn.Conv2d(256, 128, 3, 1, padding = \"same\")\n",
        "    self.deconv3_3 = nn.Conv2d(128, 128, 3, 1, padding = \"same\")\n",
        "    self.dropout_8 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 4\n",
        "    self.deconv4_1 = nn.ConvTranspose2d(128, 64, 3, padding = 1, stride = 2, output_padding = 1)\n",
        "    self.deconv4_2 = nn.Conv2d(128, 64, 3, 1, padding = 1)\n",
        "    self.deconv4_3 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.dropout_9 = nn.Dropout2d(0.2)\n",
        "\n",
        "\n",
        "    ##UP 4\n",
        "    self.deconv5_1 = nn.Conv2d(64, 64, 3, 1, padding = \"same\")\n",
        "    self.deconv5_2 = nn.Conv2d(64, n_classes, 1, 1, padding = \"same\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.Batch_Norm(x)\n",
        "    L1_1 = self.conv_1_1(x)\n",
        "    L1_1 = F.relu(L1_1)\n",
        "    L1_2 = self.conv_1_2(L1_1)\n",
        "    L1_2 = F.relu(L1_2)\n",
        "    L1_2 = self.dropout_1(L1_2)\n",
        "\n",
        "\n",
        "    L1_MAX, L1_indices = self.max_pool_1(L1_2)\n",
        "\n",
        "    L2_1 = self.conv_2_1(L1_MAX)\n",
        "    L2_1 = F.relu(L2_1)\n",
        "    L2_2 = self.conv_2_2(L2_1)\n",
        "    L2_2 = F.relu(L2_2)\n",
        "    L2_2 = self.dropout_2(L2_2)\n",
        "\n",
        "    L2_MAX, L2_indices = self.max_pool_2(L2_2)\n",
        "\n",
        "    L3_1 = self.conv_3_1(L2_MAX)\n",
        "    L3_1 = F.relu(L3_1)\n",
        "    L3_2 = self.conv_3_2(L3_1)\n",
        "    L3_2 = F.relu(L3_2)\n",
        "    L3_2 = self.dropout_3(L3_2)\n",
        "\n",
        "    L3_MAX, L3_indices = self.max_pool_3(L3_2)\n",
        "\n",
        "    L4_1 = self.conv_4_1(L3_MAX)\n",
        "    L4_1 = F.relu(L4_1)\n",
        "    L4_2 = self.conv_4_2(L4_1)\n",
        "    L4_2 = F.relu(L4_2)\n",
        "    L4_2 = self.dropout_4(L4_2)\n",
        "\n",
        "    L4_MAX, L4_indices = self.max_pool_4(L4_2)\n",
        "\n",
        "    L5_1 = self.conv_5_1(L4_MAX)\n",
        "    L5_1 = F.relu(L5_1)\n",
        "    L5_2 = self.conv_5_2(L5_1)\n",
        "    L5_2 = F.relu(L5_2)\n",
        "    L5_2 = self.dropout_5(L5_2)\n",
        "\n",
        "    ## Expansive Path\n",
        "\n",
        "    D1_1 = self.deconv1_1(L5_2)\n",
        "    skip_connection_1 = torch.cat([L4_2, D1_1], dim = 1)\n",
        "    D1_2 = self.deconv1_2(skip_connection_1)\n",
        "    D1_2 = F.relu(D1_2)\n",
        "    D1_3 = self.deconv1_3(D1_2)\n",
        "    D1_3 = F.relu(D1_3)\n",
        "    D1_3 = self.dropout_6(D1_3)\n",
        "\n",
        "    D2_1 = self.deconv2_1(D1_2)\n",
        "    skip_connection_2 = torch.cat([L3_2, D2_1], dim = 1)\n",
        "    D2_2 = self.deconv2_2(skip_connection_2)\n",
        "    D2_2 = F.relu(D2_2)\n",
        "    D2_3 = self.deconv2_3(D2_2)\n",
        "    D2_3 = F.relu(D2_3)\n",
        "    D2_3 = self.dropout_7(D2_3)\n",
        "\n",
        "    D3_1 = self.deconv3_1(D2_2)\n",
        "    skip_connection_3 = torch.cat([L2_2, D3_1], dim = 1)\n",
        "    D3_2 = self.deconv3_2(skip_connection_3)\n",
        "    D3_2 = F.relu(D3_2)\n",
        "    D3_3 = self.deconv3_3(D3_2)\n",
        "    D3_3 = F.relu(D3_3)\n",
        "    D3_3 = self.dropout_8(D3_3)\n",
        "\n",
        "    D4_1 = self.deconv4_1(D3_2)\n",
        "    skip_connection_4 = torch.cat([L1_2, D4_1], dim = 1)\n",
        "    D4_2 = self.deconv4_2(skip_connection_4)\n",
        "    D4_2 = F.relu(D4_2)\n",
        "    D4_3 = self.deconv4_3(D4_2)\n",
        "    D4_3 = F.relu(D4_3)\n",
        "    D4_3 = self.dropout_9(D4_3)\n",
        "\n",
        "    D5_1 = self.deconv5_1(D4_2)\n",
        "    D5_2 = self.deconv5_2(D5_1)\n",
        "    D5_2 = torch.sigmoid(D5_2)\n",
        "\n",
        "\n",
        "\n",
        "    return D5_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7OjU9KVCga8f"
      },
      "outputs": [],
      "source": [
        "NET = UNET()\n",
        "NET = NET.to(device)\n",
        "Optimizer = torch.optim.Adam(NET.parameters(), lr = NET.learning_rate)\n",
        "Loss = nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 5\n",
        "num_batches = 100 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KfzdiEie0Ctd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "outputId": "c82a0641-aebe-4e2f-f5cf-0847179558d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Number:  1  /  186  LOSS: 0\n",
            "Batch Number:  11  /  186  LOSS: 22.69925632350578\n",
            "Batch Number:  21  /  186  LOSS: 21.70202403792041\n",
            "Batch Number:  31  /  186  LOSS: 18.403864656855717\n",
            "Batch Number:  41  /  186  LOSS: 18.26359882470331\n",
            "Batch Number:  51  /  186  LOSS: 18.445530709161517\n",
            "Batch Number:  61  /  186  LOSS: 18.33069556389046\n",
            "Batch Number:  71  /  186  LOSS: 18.32997782381915\n",
            "Batch Number:  81  /  186  LOSS: 18.2944535176141\n",
            "Batch Number:  91  /  186  LOSS: 18.326999100239846\n",
            "Batch Number:  101  /  186  LOSS: 18.50777332039297\n",
            "Batch Number:  111  /  186  LOSS: 18.332332352480762\n",
            "Batch Number:  121  /  186  LOSS: 18.37590564949096\n",
            "Batch Number:  131  /  186  LOSS: 18.29008197907706\n",
            "Batch Number:  141  /  186  LOSS: 18.310634329643108\n",
            "Batch Number:  151  /  186  LOSS: 18.35848689531781\n",
            "Batch Number:  161  /  186  LOSS: 18.341594089205614\n",
            "Batch Number:  171  /  186  LOSS: 18.382817037311725\n",
            "Batch Number:  181  /  186  LOSS: 18.35896867499332\n",
            "Epoch #1 complete, LOSS: 349.08208299749447\n",
            "Batch Number:  1  /  186  LOSS: 0\n",
            "Batch Number:  11  /  186  LOSS: 18.333415706672213\n",
            "Batch Number:  21  /  186  LOSS: 18.385199509474205\n",
            "Batch Number:  31  /  186  LOSS: 18.34585837014413\n",
            "Batch Number:  41  /  186  LOSS: 18.263232963422183\n",
            "Batch Number:  51  /  186  LOSS: 18.444445370337462\n",
            "Batch Number:  61  /  186  LOSS: 18.331389524498945\n",
            "Batch Number:  71  /  186  LOSS: 18.329945047340175\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a529fcb24e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mEpoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mgroup_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "NET.train()\n",
        "for epoch in range(EPOCHS):\n",
        "  Epoch_loss = 0\n",
        "  group_loss = 0\n",
        "  for i, (x,y) in enumerate(train_loader):\n",
        "\n",
        "    if (i % 10 == 0):\n",
        "      print(\"Batch Number: \", i + 1, \" / \", len(train_loader), \" LOSS: \" + str(group_loss))\n",
        "      group_loss = 0 \n",
        "    \n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = NET(x)\n",
        "\n",
        "    Optimizer.zero_grad()\n",
        "    loss = Loss(output, y)\n",
        "    loss.backward()\n",
        "    Optimizer.step()\n",
        "\n",
        "    Epoch_loss += loss.item()\n",
        "    group_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch #{epoch + 1 } complete, LOSS: {Epoch_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwW_V-ChL93w"
      },
      "outputs": [],
      "source": [
        "class UNETDatasetTest(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_dir  = \"cityscapes_data/val\"\n",
        "    self.image_names = next(os.walk(self.train_dir))[2]\n",
        "    self.number_of_images = len(self.image_names)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.number_of_images\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      image_name = self.image_names[index]\n",
        "      image_data = Image.open(f\"{self.train_dir}/{image_name}\")\n",
        "\n",
        "      tensor_transform = transforms.ToTensor()\n",
        "      resize_transform = transforms.Resize((64,128))\n",
        "\n",
        "      image_data = tensor_transform(image_data)\n",
        "      image_data = resize_transform(image_data)\n",
        "\n",
        "      # img = image_data[: , : ,  : 256 ]\n",
        "      # mask = image_data[: , : , 256 : ]\n",
        "\n",
        "      img = image_data[: , : ,  : 64 ]\n",
        "      mask = image_data[: , : , 64 : ]\n",
        "\n",
        "      return img, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcvlzWgXMfo-"
      },
      "outputs": [],
      "source": [
        "# torch.save(NET.state_dict, \"/saved_UNET_64x64\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTC0z-S1MFNR"
      },
      "outputs": [],
      "source": [
        "testset = UNETDatasetTest()\n",
        "NET.eval()\n",
        "\n",
        "x,y = testset[34]\n",
        "x = x.unsqueeze(dim = 0)\n",
        "output = NET(x.to(device))\n",
        "img = output.squeeze(dim = 0).permute(1,2,0).detach().cpu().numpy()\n",
        "# img = x.permute(1,2,0).detach().numpy()\n",
        "mask = y.permute(1,2,0).detach().numpy()\n",
        "\n",
        "pixels = np.concatenate((img, mask), axis = 1)\n",
        "plt.imshow(pixels)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "U-Net",
      "provenance": [],
      "authorship_tag": "ABX9TyMZtaEVMRNPzHptr9PheoGR",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}