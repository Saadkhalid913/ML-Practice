{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionLinesModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLlRJMvf1B6FceCgH8Ss/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saadkhalid913/ML-Practice/blob/main/EmotionLinesModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRSEHYqjjwae"
      },
      "source": [
        "## importing standard modules for NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkebXECviKNy",
        "outputId": "31a25d11-7b0c-4bdc-d6c1-eedd8d76f247"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import wordnet, WordNetLemmatizer \n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buSeDuzkj1Ob"
      },
      "source": [
        "## Getting data from CSV and separating features & labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLitdmiOiikX",
        "outputId": "a3ee7fc3-4d29-4049-f87e-508ba61a788e"
      },
      "source": [
        "data = pd.read_csv(\"output.csv\", header=None, delimiter=\"~\").values\n",
        "x = data[ : , : -1]\n",
        "y = data[ : , -1 : ]\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10558, 1)\n",
            "(10558, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruv9sydqj6vj"
      },
      "source": [
        "Cleaning function for data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWIaqJXkj6ZR"
      },
      "source": [
        "EnglishStopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "def CleanData(sentences):\n",
        "  lemma = WordNetLemmatizer()\n",
        "  '''\n",
        "    Takes a 2D numpy array of sentences of \n",
        "    shape (n, 1) where n is the number of \n",
        "    sentences. Returns sentences without \n",
        "    stopwords, non-alphanumeric characters, \n",
        "    and performs lemmatization\n",
        "  '''\n",
        "  cleaned = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence[0]\n",
        "    sentence = re.sub(\"^a-zA-Z\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.split()\n",
        "    sentence = [lemma.lemmatize(word) for word in sentence if word not in EnglishStopwords]\n",
        "    sentence = \" \".join(sentence)\n",
        "    cleaned.append(sentence)\n",
        "\n",
        "  return cleaned \n",
        "\n",
        "x = CleanData(x) # cleaning features "
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l_E88THlYaq"
      },
      "source": [
        "## applying tokenization to features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFhvNYMMlXQ0"
      },
      "source": [
        "def FitAndTokenize(sentences):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "  tokenizer.fit_on_texts(sentences)\n",
        "  result = tokenizer.texts_to_sequences(sentences)\n",
        "  result = tf.keras.preprocessing.sequence.pad_sequences(result, maxlen=90)\n",
        "  return result, tokenizer \n",
        "\n",
        "def Tokenize(sentences, tokenizer):\n",
        "  result = tokenizer.texts_to_sequences(sentences)\n",
        "  result = tf.keras.preprocessing.sequence.pad_sequences(result, maxlen=90)\n",
        "  return result \n",
        "\n",
        "x, tokenizer = FitAndTokenize(x)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZljajYWpFjK"
      },
      "source": [
        "num_words = len(tokenizer.index_word) + 1"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbdtp-KAn0UA"
      },
      "source": [
        "Encoding labels using one hot vector encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOvjsyo-l5BL"
      },
      "source": [
        "encoder = OneHotEncoder()\n",
        "y = encoder.fit_transform(y).toarray()\n",
        "KEY = encoder.categories_[0]\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NU26tNOoXSl"
      },
      "source": [
        "def CreateModel():\n",
        "  '''\n",
        "    creating tf.keras model \n",
        "  '''\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(num_words, 240, input_length=90))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=240, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation=\"relu\"))\n",
        "  model.add(tf.keras.layers.Dense(units=8, activation=\"softmax\"))\n",
        "  model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "  return model \n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPRFwphEp343"
      },
      "source": [
        "ann = CreateModel()\n",
        "ann.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSO53g6iqsJ3",
        "outputId": "57a00b60-8a86-4afa-b690-09ec6dbb905e"
      },
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10558, 90)\n",
            "(10558, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xybJbRrqcAF",
        "outputId": "bacd5110-111c-4900-d2bf-70cded8bc2ea"
      },
      "source": [
        "ann.fit(x, y, epochs=25, batch_size=100)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "106/106 [==============================] - 11s 96ms/step - loss: 1.6077 - accuracy: 0.4418\n",
            "Epoch 2/25\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 1.4317 - accuracy: 0.5027\n",
            "Epoch 3/25\n",
            "106/106 [==============================] - 10s 96ms/step - loss: 1.0989 - accuracy: 0.6235\n",
            "Epoch 4/25\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.8045 - accuracy: 0.7247\n",
            "Epoch 5/25\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.6339 - accuracy: 0.7855\n",
            "Epoch 6/25\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.5299 - accuracy: 0.8219\n",
            "Epoch 7/25\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.4604 - accuracy: 0.8378\n",
            "Epoch 8/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.4196 - accuracy: 0.8536\n",
            "Epoch 9/25\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.3920 - accuracy: 0.8633\n",
            "Epoch 10/25\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.3639 - accuracy: 0.8704\n",
            "Epoch 11/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.3516 - accuracy: 0.8764\n",
            "Epoch 12/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.3312 - accuracy: 0.8810\n",
            "Epoch 13/25\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.3216 - accuracy: 0.8846\n",
            "Epoch 14/25\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.3081 - accuracy: 0.8892\n",
            "Epoch 15/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.2990 - accuracy: 0.8889\n",
            "Epoch 16/25\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.2902 - accuracy: 0.8880\n",
            "Epoch 17/25\n",
            "106/106 [==============================] - 10s 99ms/step - loss: 0.2856 - accuracy: 0.8951\n",
            "Epoch 18/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.2828 - accuracy: 0.8956\n",
            "Epoch 19/25\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.2689 - accuracy: 0.8988\n",
            "Epoch 20/25\n",
            "106/106 [==============================] - 10s 98ms/step - loss: 0.2734 - accuracy: 0.8961\n",
            "Epoch 21/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.2620 - accuracy: 0.8996\n",
            "Epoch 22/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.2591 - accuracy: 0.9007\n",
            "Epoch 23/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.2581 - accuracy: 0.9013\n",
            "Epoch 24/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.2564 - accuracy: 0.9053\n",
            "Epoch 25/25\n",
            "106/106 [==============================] - 10s 97ms/step - loss: 0.2398 - accuracy: 0.9086\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f00cd737d10>"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    }
  ]
}